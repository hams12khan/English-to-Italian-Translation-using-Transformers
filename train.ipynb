{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rp273/anaconda3/envs/Hammad/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import build_transformer\n",
    "from dataset import BilingualDataset, causal_mask\n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "\n",
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "def get_ds(config):\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA A10\n",
      "Device memory: 21.988037109375 GB\n",
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 910/910 [09:03<00:00,  1.68it/s, loss=6.012]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'It would have been worse still on wheels, Constantine Dmitrich,' said the driver, whom Levin knew.\n",
      "    TARGET: — In carrozza è anche peggio, Konstantin Dmitric — rispose il postiglione che lo conosceva.\n",
      " PREDICTED: — E non è detto , — disse Levin , — disse Levin . — E il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo ’ ic .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was bitterly cold. The wind cut like a knife.\n",
      "    TARGET: Faceva un gran freddo, e il vento tagliava come un coltello.\n",
      " PREDICTED: , e la sua sua sua sua sua sua sua sua .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 910/910 [09:04<00:00,  1.67it/s, loss=5.867]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Oh, it makes me sick to hear it!' muttered the Prince gloomily, rising as if he meant to go away, but stopping at the door. 'The laws are there, my dear, and since you have invited it I will tell you who is at fault for it all: you, and you, and no one but you!\n",
      "    TARGET: — Ah, se aveste dato retta a me! — esclamò cupo il principe, alzandosi dalla poltrona e desiderando andarsene; ma, fermandosi poi sulla porta: — Le leggi! ci sono, matuška, e giacché tu mi stai provocando, ti dirò che la colpa di tutto questo è tua, tua, tua soltanto.\n",
      " PREDICTED: — Sì , vi ho detto che vi è detto che è detto che è detto che è un po ’ di un po ’ di nuovo , e che non è detto che non è fatto che è fatto che è fatto , e non è fatto che è fatto che è fatto , e non è detto che è fatto che è fatto che vi è fatto .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'After all, they're all human beings, all men, just like us poor sinners,' he thought, as he entered the hotel. 'What is there to be angry and quarrel about?'\n",
      "    TARGET: “Tutti sono esseri umani, uomini, come noi, peccatori; perché arrabbiarsi e litigare?” pensava, entrando nell’albergo.\n",
      " PREDICTED: — E , , , , , che è un po ’ di , — disse , — e che non è stato stato stato di lei , e che non è stato stato ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 910/910 [09:04<00:00,  1.67it/s, loss=5.205]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Dorchester is a delightfully peaceful old place, nestling in stillness and silence and drowsiness.\n",
      "    TARGET: Dorchester è un luogo deliziosamente tranquillo, annidato nella calma, nel silenzio e nella sonnolenza.\n",
      " PREDICTED: E un ’ altra parte di un ’ altra parte , un ’ altra , e si e si .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Well, at least your skeleton is a funny one and not a dismal one,' said Dolly smiling. 'No, it is a dismal one. Do you know why I am going to-day and not to-morrow?\n",
      "    TARGET: — Ma certamente sono gai, i tuoi skeletons, non tenebrosi — disse sorridendo Dolly.\n",
      " PREDICTED: — E tu , è un uomo che è un uomo che non è un sorriso — disse Stepan Arkad ’ ic , sorridendo . — Non è vero che non è vero ? — disse . — Non è vero ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 910/910 [09:04<00:00,  1.67it/s, loss=5.073]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Sight-seeing, apart from the fact that he had already seen everything, had for him – a Russian and an intelligent man – none of that inexplicable importance the English manage to attach to it.\n",
      "    TARGET: La visita ai monumenti più importanti, oltre al fatto che tutto era stato visitato, non aveva per lui, russo e uomo d’ingegno, quell’inspiegabile importanza che le attribuiscono gli inglesi.\n",
      " PREDICTED: , , per la sua vita , era che era stato un ’ altra , che non aveva mai un ’ uomo che non si era stato , e che non si a .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But that sorrow and this joy were equally beyond the usual conditions of life: they were like openings in that usual life through which something higher became visible.\n",
      "    TARGET: Ma sia quel dolore che questa gioia erano egualmente al di fuori di tutte le solite contingenze della vita ed erano, nella consuetudine della vita, come uno spiraglio attraverso il quale appariva qualcosa di ultraterreno.\n",
      " PREDICTED: Ma che si e la sua vita era la sua vita della vita , che si erano stati in cui , per la vita , che si era stato stata in cui si era stato stato stata in cui si era stato .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 910/910 [09:03<00:00,  1.67it/s, loss=5.024]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Heavy showers, with thunderstorms, may be expected to-day,\" it would say on Monday, and so we would give up our picnic, and stop indoors all day, waiting for the rain. - And people would pass the house, going off in wagonettes and coaches as jolly and merry as could be, the sun shining out, and not a cloud to be seen.\n",
      "    TARGET: «Oggi c’è probabilità di grossi acquazzoni con fulmini» stampò il giornale un lunedì; e noi rinunziammo alla scampagnata, e ci fermammo in casa aspettando la pioggia. E la gente passava sotto le finestre, riempiendo i calessi e le carrozze, più allegra che mai, con un sole fulgidissimo e neppure una nuvoletta.\n",
      " PREDICTED: — , , — disse , — che ci di , e ci , e ci di , e ci , e di , e , e , , e , , e non si .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"A strapper--a real strapper, Jane: big, brown, and buxom; with hair just such as the ladies of Carthage must have had.\n",
      "    TARGET: — È bella, forte, bruna e snella, con i capelli come dovevano averli le donne di Cartagine; ma ecco Dent e Lynn nelle scuderie; entrate in casa da questa porta.\n",
      " PREDICTED: — Un ' altra cosa , Jane , Jane , e il collo , e il collo , come i capelli neri , che vi hanno .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 910/910 [09:04<00:00,  1.67it/s, loss=4.920]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He could not explain what it was that had so moved him; he was sorry for her and felt that he could not help her, because he knew that he was the cause of her trouble, that he had done wrong.\n",
      "    TARGET: Non avrebbe potuto dire che cosa proprio l’avesse commosso tanto; aveva pena di lei e sentiva che non poteva aiutarla, mentre egli era colpevole dell’infelicità sua, egli le aveva fatto del male.\n",
      " PREDICTED: Non poteva dire che cosa fosse stato così così , e lui gli era stato felice di lei , perché non poteva non capire perché quello che egli aveva detto , perché era stato stato stato la sua situazione , che era stato accaduto .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: As soon as we came clear of the trees, which blinded us before, we saw clearly what had been the case, and how Friday had disengaged the poor guide, though we did not presently discern what kind of creature it was he had killed.\n",
      "    TARGET: Appena fummo fuor degli alberi che ne toglievano dianzi la vista, scorgemmo perfettamente il caso, e come Venerdì fosse riuscito a campare da morte il nostro povero conduttore, benchè l’oscurità dell’ora non ne lasciasse allora discernere qual razza di bestia egli avesse uccisa.\n",
      " PREDICTED: Appena avevamo di , che , per , , si trovava in quel momento , e che , dopo aver fatto il capitano , se non ne fosse stato , se ne fosse stato stato fatto , se ne fosse stato fatto , o se ne fosse stato fatto .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=4.471]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was always painful for Kitty to part from her husband for two days; but seeing his animated figure, which seemed particularly large and powerful in high shooting boots and white blouse, and the radiant exhilaration of the sportsman in him, incomprehensible to her, she forgot her own pain in his gladness and parted from him cheerfully.\n",
      "    TARGET: Per Kitty, come sempre, era doloroso separarsi dal marito per due giorni, ma, vista la figura di lui animata, che sembrava ancor più grande e forte con gli stivaloni e il camiciotto bianco, e una certa luce negli occhi, per lei incomprensibile, dovuta alla eccitazione della caccia, per questa sua gioia dimenticò il proprio cruccio e lo congedò allegra.\n",
      " PREDICTED: Era sempre sempre per Kitty per l ’ incontro per le due ore ; ma , in quel momento , in cui , in cui , in alto , aveva sentito , i capelli , i , e i , , la sua vita , la sua vita , la sua vita , la sua , la sua vita , con lui , con lui , con la sua vita .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She only knew that she had to call on Madame Berthe and get home in time for Mama's midnight tea.\n",
      "    TARGET: Ricordava solo che quel giorno doveva ancora passare da m.me Berthe e che doveva arrivare in tempo a casa per il tè di maman, verso mezzanotte.\n",
      " PREDICTED: Ella sapeva che la signora Fairfax e la signora Fairfax e la signora Fairfax .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=4.681]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Montmorency went for that poor cat at the rate of twenty miles an hour; but the cat did not hurry up - did not seem to have grasped the idea that its life was in danger.\n",
      "    TARGET: Montmorency si scagliò dietro quel povero gatto alla velocità di venti miglia all’ora; ma il gatto non si mise a correre — parve non gli fosse lampeggiata l’idea che la sua vita era in pericolo.\n",
      " PREDICTED: Montmorency andò a prendere un po ’ di giorni , ma il gatto non si fermò , ma il gatto non si poteva non pensare che la sua vita fosse stata la vita di prima .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: During their engagement he had been struck by the definiteness with which she declined a trip abroad and decided to go to the country, as if she knew of something that was necessary, and could think of something besides their love.\n",
      "    TARGET: Già quand’era stato fidanzato, si era sorpreso della sicurezza con cui ella aveva rinunciato al viaggio all’estero e aveva deciso di andare in campagna, quasi avesse avuto in mente qualcosa che si doveva fare, e quasi ella potesse pensare, oltre al suo amore, a un qualcosa che ne fosse al di fuori.\n",
      " PREDICTED: Durante il loro lavoro , il pensiero del pensiero , il quale aveva già fatto in campagna e , come si fosse andato a casa , come se fosse stato stato un ’ altra cosa , fosse stato possibile qualcosa di buono .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=4.179]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Well, is he?\"\n",
      "    TARGET: — Vi pare dunque bizzarro?\n",
      " PREDICTED: — Ebbene , è vero ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Some people said that she had made for herself a position in Society by her pose as a philanthropic and highly religious woman; others said that she really was the highly moral being, living only to do good, that she seemed to be.\n",
      "    TARGET: La signora Stahl viveva da più di dieci anni continuamente all’estero, al sud, senza mai alzarsi dal letto. Alcuni dicevano che la signora Stahl si era creata in società la fama di donna virtuosa, profondamente religiosa; altri dicevano ch’ella era tale nell’anima quale appariva: un essere altamente morale che viveva solo per il bene del prossimo.\n",
      " PREDICTED: Alcuni dicono che , per lei , aveva avuto la sua posizione in società , aveva un ’ altra donna con una donna e una donna che aveva detto , aveva capito che non solo per questo , era impossibile , che ella fosse la propria persona .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 910/910 [09:04<00:00,  1.67it/s, loss=3.811]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"You little sharp thing! you've got quite a new way of talking.\n",
      "    TARGET: — Aspra creatura, ecco una nuova maniera di parlare.\n",
      " PREDICTED: — Avete fatto un po ' di nuovo , — disse con un nuovo nuovo nuovo nuovo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And I added that I hoped she understood that it had nothing to do with me; and she said that she was sure of that, but that she would speak to Tom about it when he came back.\n",
      "    TARGET: E aggiunsi che m’auguravo ch’ella comprendesse che io non ci entravo; ed ella mi disse che certo così era; ma che Tommaso al suo ritorno l’avrebbe sentita.\n",
      " PREDICTED: E aggiunse che non aveva mai capito nulla di che non aveva nulla di nulla . E poi mi disse che era impossibile , ma che , quando si sarebbe detto , si sarebbe detto di dire che si sarebbe messo a piangere .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=4.261]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I must say you have an appetite!' he said, glancing at the sunburnt ruddy face bent over the plate.\n",
      "    TARGET: — Eh, che appetito che hai! — disse, guardando il volto abbronzato rosso-scuro e il collo di lui chino sul piatto.\n",
      " PREDICTED: — Vi occorre un po ’, vi ho detto , guardando il mento rosso .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Well? Didn't I tell you?' said Oblonsky, who saw that Levin had been entirely vanquished.\n",
      "    TARGET: — Be’, via, te l’avevo detto — gli disse Stepan Arkad’ic, vedendo che Levin era completamente conquistato.\n",
      " PREDICTED: — Be ’, non mi pare ? — disse Stepan Arkad ’ ic che Levin vedeva che Levin era stato già stato .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=3.843]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Much better.'\n",
      "    TARGET: — Va molto meglio.\n",
      " PREDICTED: — Molto meglio .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I hated it the first time I set my eyes on it--a sickly, whining, pining thing!\n",
      "    TARGET: Dal giorno che vidi quella bambina esile e piagnucolosa, la odiai.\n",
      " PREDICTED: \" Non mi sentivo prima che io mi i miei occhi , una , che , , , , , , la prima volta .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=3.700]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And signs, for aught we know, may be but the sympathies of Nature with man.\n",
      "    TARGET: I presagi poi potrebbero essere simpatie fra la natura e l'uomo.\n",
      " PREDICTED: E i segni più lontani ci possono essere la natura della natura , ma la natura della natura si può essere .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Never mind! Tell them I will pay!' and nodding his head to an acquaintance who was driving past he disappeared round the corner.\n",
      "    TARGET: — Non fa nulla; di’ che pagherò poi — e scomparve dopo aver salutato allegramente, con un cenno del capo, un conoscente che passava.\n",
      " PREDICTED: — Non ti prego , ! — e con un cenno al capo di nuovo , che si avvicinava a un angolo .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=3.404]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Has he been snatched up to heaven?\" I queried.\n",
      "    TARGET: — È stato rapito in cielo? — domandai.\n",
      " PREDICTED: — È stato da me ? — domandai .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Yes; just so, in _your_ circumstances: but find me another precisely placed as you are.\"\n",
      "    TARGET: — Sì, nella vostra posizione; ma vi pare che vi sia una sola persona collocata esattamente nella vostra posizione?\n",
      " PREDICTED: — Sì , ma quando la vostra partenza mi è passata accanto a voi ; avete una volta l ' altra .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=3.291]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: What did St. John Rivers think of this earthly angel?\n",
      "    TARGET: Che cosa pensava Saint-John di quell'angiolo terrestre?\n",
      " PREDICTED: Che Saint - John ha detto il angelo del angelo ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'In Kolok, I expect, and Nurse is with them.'\n",
      "    TARGET: — Al Kolok, forse, e la njanja è con lui.\n",
      " PREDICTED: — Al club ne ho fatto , ho bevuto .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 910/910 [09:04<00:00,  1.67it/s, loss=2.774]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"We cannot--we cannot,\" he answered, with short, sharp determination: \"it would not do.\n",
      "    TARGET: — Non possiamo, — mi rispose con voce breve e imperiosa.\n",
      " PREDICTED: — Non possiamo , non possiamo , — rispose con molta cortesia , — non possiamo fare altro , — e non ci saremmo andati .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I am not late?'\n",
      "    TARGET: — Non sono in ritardo?\n",
      " PREDICTED: — Non sono in ritardo ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=2.925]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Whoever likes,' replied Sviyazhsky.\n",
      "    TARGET: — Chi vorrà — disse Svijazskij.\n",
      " PREDICTED: — Un ’ occhiata — rispose Svijazskij .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Come back!' the Caterpillar called after her. 'I've something important to say!'\n",
      "    TARGET: — Vieni qui! — la richiamò il Bruco. — Ho qualche cosa d'importante da dirti.\n",
      " PREDICTED: — Venite , bevi ! — gridò il Bruco dopo un po ' importante . — Ho avuto da dire :\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=2.890]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Yet he whom it describes scarcely impressed one with the idea of a gentle, a yielding, an impressible, or even of a placid nature.\n",
      "    TARGET: Eppure nel guardare il signor Saint-John non mi dette l'idea di avere un carattere gentile, cortese, sensibile e neppur placido.\n",
      " PREDICTED: Ma colui che era rimasto in me lo aveva strappato e una grande natura .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: CHAPTER I\n",
      "    TARGET: I\n",
      " PREDICTED: I\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=2.375]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was _my_ time to assume ascendency. _My_ powers were in play and in force.\n",
      "    TARGET: Ora stava a me a prendere l'ascendente. Le mie facoltà erano in giuoco ed ero piena di forze.\n",
      " PREDICTED: Era un piccolo acuto per le forze . Si sentiva in forza della forza .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He stuck to this arrangement for a couple of months, and then he grew dissatisfied with it.\n",
      "    TARGET: Osservò questa disposizione per un paio di mesi, ma poi ne divenne malcontento.\n",
      " PREDICTED: perché la figlia è per due mesi e poi si di nuovo .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 910/910 [09:05<00:00,  1.67it/s, loss=2.851]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I didn't know it was your table,' said Alice; 'it's laid for a great many more than three.'\n",
      "    TARGET: — Non sapevo che la tavola ti appartenesse, — rispose Alice; — è apparecchiata per più di tre.\n",
      " PREDICTED: — Non so che fosse la tua tavola , — rispose Alice ; — e per tre mesi fu un gran numero di tre .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'What's to be done?\n",
      "    TARGET: — Che fare?\n",
      " PREDICTED: — Che fare ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hammad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
